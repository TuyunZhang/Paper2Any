import base64
from typing import Any, Dict, List
from dataflow_agent.state import MainState
from langchain_core.messages import AIMessage, BaseMessage
import aiohttp

from dataflow_agent.llm_callers.base import BaseLLMCaller
from dataflow_agent.logger import get_logger

log = get_logger(__name__)

class VisionLLMCaller(BaseLLMCaller):
    """è§†è§‰LLMè°ƒç”¨å™¨ - æ”¯æŒå›¾åƒè¾“å…¥/è¾“å‡º"""
    
    def __init__(self, 
                 state: MainState,
                 vlm_config: Dict[str, Any],
                 **kwargs):
        """
        Args:
            vlm_config: VLMé…ç½®ï¼ŒåŒ…å«ï¼š
                - mode: "generation" | "edit" | "understanding"
                - input_image: è¾“å…¥å›¾åƒè·¯å¾„ï¼ˆedit/understandingæ¨¡å¼ï¼‰
                - output_image: è¾“å‡ºå›¾åƒä¿å­˜è·¯å¾„ï¼ˆgeneration/editæ¨¡å¼ï¼‰
                - response_format: "image" | "text" (é»˜è®¤æ ¹æ®modeè‡ªåŠ¨åˆ¤æ–­)
        """
        super().__init__(state, **kwargs)
        self.vlm_config = vlm_config
        self.mode       = vlm_config.get("mode", "understanding")
        self.temperature = kwargs.get("temperature", 0.1)
        # self.max_tokens = kwargs.get("max_tokens", 16384)
    
    async def call(self, messages: List[BaseMessage], bind_post_tools: bool = False) -> AIMessage:
        """è°ƒç”¨VLM"""
        log.info(f"VisionLLMè°ƒç”¨ï¼Œæ¨¡å‹: {self.model_name}, æ¨¡å¼: {self.mode}")
        
        if self.mode in ["generation", "edit"]:
            return await self._call_image_output(messages)
        else:
            return await self._call_image_understanding(messages)
        
    async def _call_image_understanding(self, messages: List[BaseMessage]) -> AIMessage:
        """å›¾åƒç†è§£æ¨¡å¼ - è¾“å…¥å›¾åƒï¼Œè¾“å‡ºæ–‡æœ¬"""

        # ------------------------------------------------------------------
        # åˆ†æ”¯ï¼šQwen-VL-OCR (APIYI / DashScope å…¼å®¹)
        # ------------------------------------------------------------------
        if "qwen-vl-ocr" in self.model_name.lower() and "apiyi" in self.state.request.chat_api_url:
            log.info(f"[VisionLLM] å‘½ä¸­ Qwen-VL-OCR (APIYI) ç†è§£åˆ†æ”¯: {self.model_name}")
            
            processed_messages = []
            for msg in messages:
                role = "user"
                if hasattr(msg, "type"):
                    # LangChain message type mapping
                    if msg.type == "human": role = "user"
                    elif msg.type == "ai": role = "assistant"
                    elif msg.type == "system": role = "system"
                processed_messages.append({"role": role, "content": msg.content})

            # å¤„ç†è¾“å…¥å›¾åƒ
            if "input_image" in self.vlm_config:
                img_path = self.vlm_config["input_image"]
                b64, fmt = self._encode_image(img_path)
                
                # æ‰¾åˆ°æœ€åä¸€æ¡ user æ¶ˆæ¯æ³¨å…¥å›¾ç‰‡
                target_msg = None
                for m in reversed(processed_messages):
                    if m["role"] == "user":
                        target_msg = m
                        break
                
                if target_msg:
                    original_text = target_msg["content"]
                    # æ„é€  OpenAI Vision æ ¼å¼
                    target_msg["content"] = [
                        {"type": "image_url", "image_url": {"url": f"data:image/{fmt};base64,{b64}"}},
                        {"type": "text", "text": original_text}
                    ]
                else:
                    # å¦‚æœæ²¡æœ‰ user æ¶ˆæ¯ï¼Œå¼ºè¡ŒåŠ ä¸€æ¡
                    processed_messages.append({
                        "role": "user",
                        "content": [
                            {"type": "image_url", "image_url": {"url": f"data:image/{fmt};base64,{b64}"}},
                            {"type": "text", "text": "Describe this image."}
                        ]
                    })

            # æ„é€  Payload
            payload = {
                "model": self.model_name,
                "messages": processed_messages,
                # OCR ä»»åŠ¡å»ºè®®ä½æ¸©
                "temperature": 0.01, 
                "max_tokens": self.max_tokens if hasattr(self, 'max_tokens') else 4096,
            }
            
            resp = await self._post_chat_completions(payload)
            content = resp["choices"][0]["message"]["content"]
            return AIMessage(content=content)

        # ------------------------------------------------------------------
        # é€šç”¨åˆ†æ”¯ (é»˜è®¤)
        # ------------------------------------------------------------------
        ROLE_MAP = {
            "human": "user",
            "ai": "assistant",
            "system": "system",
            "tool": "tool",
        }

        processed_messages: List[Dict[str, Any]] = []
        for msg in messages:
            lc_role = getattr(msg, "type", "human")     # human / ai / system / tool
            role     = ROLE_MAP.get(lc_role, "user")    # å…œåº•ç”¨ "user"

            processed_messages.append({
                "role": role,
                "content": msg.content                  # str æˆ– multimodal list éƒ½è¡Œ
            })

        # --------  å¦‚æœ‰ input_imageï¼ŒæŠŠå›¾è´´åˆ°æœ€åä¸€æ¡ user æ¶ˆæ¯é‡Œ --------
        if "input_image" in self.vlm_config and processed_messages:
            b64, fmt = self._encode_image(self.vlm_config["input_image"])
            last_msg = processed_messages[-1]
            if last_msg["role"] == "user":
                last_msg["content"] = [
                    {"type": "text",  "text": last_msg["content"]},
                    {"type": "image_url",
                    "image_url": {"url": f"data:image/{fmt};base64,{b64}"}}
                ]

        payload = {
            "model": self.model_name,
            "messages": processed_messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }
        response_data = await self._post_chat_completions(payload)
        # log.critical(f"VisionLLMç†è§£æ¨¡å¼å“åº”: {response_data}")
        content = response_data["choices"][0]["message"]["content"]
        return AIMessage(content=content)
    
    # async def _call_image_understanding(self, messages: List[BaseMessage]) -> AIMessage:
    #     """å›¾åƒç†è§£æ¨¡å¼ - è¾“å…¥å›¾åƒï¼Œè¾“å‡ºæ–‡æœ¬"""
    #     # è¿™ä¸ªè¿˜æœ‰bugï¼ï¼ï¼


    #     import httpx
        
    #     # æ„å»ºåŒ…å«å›¾åƒçš„æ¶ˆæ¯
    #     processed_messages = []
    #     for msg in messages:
    #         if hasattr(msg, 'content') and isinstance(msg.content, str):
    #             processed_messages.append({
    #                 "role": msg.type if hasattr(msg, 'type') else "user",
    #                 "content": msg.content
    #             })
        
    #     # å¦‚æœé…ç½®äº†è¾“å…¥å›¾åƒï¼Œæ·»åŠ åˆ°æœ€åä¸€æ¡æ¶ˆæ¯
    #     if "input_image" in self.vlm_config:
    #         b64, fmt = self._encode_image(self.vlm_config["input_image"])
            
    #         # ä¿®æ”¹æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä¸ºå¤šæ¨¡æ€æ ¼å¼
    #         last_msg = processed_messages[-1]
    #         if last_msg["role"] == "user":
    #             last_msg["content"] = [
    #                 {"type": "text", "text": last_msg["content"]},
    #                 {"type": "image_url", 
    #                  "image_url": {"url": f"data:image/{fmt};base64,{b64}"}}
    #             ]
        
    #     # è°ƒç”¨API
    #     payload = {
    #         "model": self.model_name,
    #         "messages": processed_messages,
    #         "temperature": self.temperature,
    #         "max_tokens": self.max_tokens,
    #     }
        
    #     response_data = await self._post_chat_completions(payload)
    #     content = response_data["choices"][0]["message"]["content"]
        
    #     return AIMessage(content=content)
    
    async def _call_image_output(self, messages: List[BaseMessage]) -> AIMessage:
        """å›¾åƒç”Ÿæˆ/ç¼–è¾‘æ¨¡å¼ - è¾“å‡ºå›¾åƒ"""
        from dataflow_agent.toolkits.imtool.req_img import generate_or_edit_and_save_image_async
        
        # æå–promptï¼ˆæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼‰
        prompt = ""
        for msg in reversed(messages):
            if hasattr(msg, 'content'):
                prompt = msg.content
                break
        
        # è°ƒç”¨å›¾åƒç”Ÿæˆå‡½æ•°
        save_path = self.vlm_config.get("output_image", "./generated_image.png")
        image_path = self.vlm_config.get("input_image") if self.mode == "edit" else None
        aspect_ratio = self.vlm_config.get("aspect_ratio", "16:9")  # <-- æ–°å¢ï¼šé»˜è®¤ 16:9
        
        b64 = await generate_or_edit_and_save_image_async(
            prompt=prompt,
            save_path=save_path,
            api_url=self.state.request.chat_api_url,
            api_key=self.state.request.api_key,
            model=self.model_name,
            image_path=image_path,
            use_edit=(self.mode == "edit"),
            timeout=self.vlm_config.get("timeout", 120),
            aspect_ratio = aspect_ratio 
        )
        
        # è¿”å›å›¾åƒè·¯å¾„ä½œä¸ºå†…å®¹
        content = f"å›¾åƒå·²ç”Ÿæˆå¹¶ä¿å­˜è‡³: {save_path}"
        
        return AIMessage(content=content, additional_kwargs={
            "image_path": save_path,
            "image_base64": b64,
        })
    
    def _encode_image(self, image_path: str) -> tuple:
        """ç¼–ç å›¾åƒä¸ºbase64"""
        with open(image_path, "rb") as f:
            raw = f.read()
        b64 = base64.b64encode(raw).decode("utf-8")
        
        ext = image_path.rsplit(".", 1)[-1].lower()
        if ext in {"jpg", "jpeg"}:
            fmt = "jpeg"
        elif ext == "png":
            fmt = "png"
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å›¾åƒæ ¼å¼: {ext}")
        
        return b64, fmt
    
    # async def _post_chat_completions(self, payload: dict) -> dict:
    #     """è°ƒç”¨chat completions API"""
    #     import httpx
        
    #     url = f"{self.state.request.chat_api_url}/chat/completions".rstrip("/")

    #     log.critical(f"[_post_chat_completions]>è°ƒç”¨URL: {url}")
    #     headers = {
    #         "Authorization": f"Bearer {self.state.request.api_key}",
    #         "Content-Type": "application/json",
    #     }
        
    #     timeout = self.vlm_config.get("timeout", 120)
    #     async with httpx.AsyncClient(timeout=httpx.Timeout(timeout)) as client:
    #         resp = await client.post(url, headers=headers, json=payload)
    #         resp.raise_for_status()
    #         return resp.json()

    async def _post_chat_completions(self, payload: dict) -> dict:
        """è°ƒç”¨chat completions API"""
        import httpx
        
        # æ„å»ºURL
        base_url = self.state.request.chat_api_url.rstrip("/")
        url = f"{base_url}/chat/completions"
        
        # æ—¥å¿—è®°å½•
        log.critical(f"[è°ƒç”¨URL]: {url}")
        log.debug(f"[è¯·æ±‚payload]: {payload}")
        
        headers = {
            "Authorization": f"Bearer {self.state.request.api_key}",
            "Content-Type": "application/json",
        }
        
        # å¤„ç†timeoutå‚æ•°
        timeout_value = self.vlm_config.get("timeout", 120)
        if isinstance(timeout_value, str):
            try:
                timeout = float(timeout_value)
            except ValueError:
                log.warning(f"timeouté…ç½®'{timeout_value}'æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼120")
                timeout = 120
        else:
            timeout = float(timeout_value)
        
        log.info(f"[è¯·æ±‚é…ç½®] timeout={timeout}s")
        
        try:
            # åˆ›å»ºå®¢æˆ·ç«¯æ—¶æ˜¾å¼é…ç½®ï¼Œé¿å…ç¯å¢ƒå˜é‡å¹²æ‰°
            async with httpx.AsyncClient(
                timeout=httpx.Timeout(timeout),
                proxy=None,        # ç¦ç”¨ä»£ç†
                trust_env=False,     # ä¸è¯»å–ç¯å¢ƒå˜é‡
                follow_redirects=True
            ) as client:
                resp = await client.post(url, headers=headers, json=payload)
                resp.raise_for_status()
                data = resp.json()
                log.debug(f"[å“åº”]: {data}")
                return data
                
        except httpx.HTTPStatusError as e:
            log.error(f"HTTPé”™è¯¯ {e.response.status_code}: {e.response.text}")
            raise
        except httpx.RequestError as e:
            log.error(f"è¯·æ±‚é”™è¯¯: {e}")
            raise
        except Exception as e:
            log.error(f"æœªçŸ¥é”™è¯¯: {e}")
            raise
        
# ======================================================================
# å¿«é€Ÿè‡ªæµ‹ï¼špython vision.py <image_path>
# ======================================================================
if __name__ == "__main__":
    """
    ç”¨æ³•:
        python vision.py /path/to/your/image.png
    """
    import os
    import sys
    import asyncio
    from types import SimpleNamespace
    from pathlib import Path
    from langchain_core.messages import HumanMessage

    async def _quick_test(img_path: str):
        # 1. ç¯å¢ƒå˜é‡æ£€æŸ¥
        api_url = os.getenv("DF_API_URL")
        api_key = os.getenv("DF_API_KEY")
        if not api_url or not api_key:
            print("âŒ  è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ DF_API_URL / DF_API_KEY")
            sys.exit(1)

        # 2. æ£€æŸ¥å›¾ç‰‡
        img_path = Path(img_path).expanduser().resolve()
        if not img_path.exists():
            print(f"âŒ  å›¾ç‰‡ä¸å­˜åœ¨: {img_path}")
            sys.exit(1)

        # 3. æ„é€ æç®€ MainState
        request = SimpleNamespace(chat_api_url=api_url.rstrip("/"), api_key=api_key, model = "gemini-2.5-flash-image-preview")
        state = SimpleNamespace(request=request)

        # 4. å®ä¾‹åŒ–å¹¶è°ƒç”¨
        caller = VisionLLMCaller(
            state=state,
            vlm_config={
                "mode": "generation",
                "input_image": str(img_path),
                "timeout": 60,
            },
            
        )
        print("ğŸš€ æ­£åœ¨è¯·æ±‚æ¨¡å‹ï¼Œè¯·ç¨å€™ â€¦")
        ai_msg = await caller.call([HumanMessage(content="æ°´æœé£æ ¼çŒ«ï¼")])

        print("\n================  ç»“æœ  ================")
        print(ai_msg.content)
        print("========================================")

    # -------- å…¥å£ --------
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python vision.py <image_path>")
        sys.exit(0)

    asyncio.run(_quick_test(sys.argv[1]))
